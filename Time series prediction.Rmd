---
title: "Data Exploration"
author: "Gianluca Scuri"
date: "2022-12-11"
output:
  pdf_document: default
  html_document: default
---

# Time series forecast

## 0. Import

### Setup and useful functions

```{r}
set.seed(100)

Sys.setenv(TZ='GMT') # imposto la time zone

packages <- c("forecast", "KFAS", "xts", "fastDummies", "tsfknn", "MASS", "tidyr", "ggplot2", "lubridate", "randomForest", "ranger", "tibble") # librerie

installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Packages loading
invisible(lapply(packages, library, character.only = TRUE))
```

```{r}
stats <- function(actual,pred){
  rmse <- sqrt(mean((actual - pred)^2))
  mape <- mean(abs((actual - pred)/actual))*100
  mae <- mean(abs(actual - pred))
  cat("RMSE", rmse, "\nMAPE", mape, "\nMAE ", mae, "\n")
  return(c(rmse, mape, mae))
}

# SignificativitÃ¡ regressori
pars_test <- function(coef, var_coef){
  test <- (1-pnorm(abs(coef)/sqrt(diag(var_coef))))*2
  return(test)
}
```

```{r}
plot_pred <- function(pred, days=3){
  diff <- plot(data_xts[(val_index-144*5):(val_index+144*npred-1)]-pred) # serie storica - storica
  plot <- plot(data_xts[(val_index-144*5):(val_index+144*npred-1)], lwd=3) # serie storica e storica
  plot <- lines(pred, type = "l", col = "red", lwd=3)
  zoom <- plot(data_xts[(val_index+144*(npred-days)):(val_index+144*npred-1)], lwd=3) # zoom serie storica e storica
  zoom <- lines(pred, type = "l", col = "red", lwd=3)
  c(diff, plot, zoom)
}
```

```{r}
theme_new <- theme_set(theme_light())
```

### Loading data

```{r}
working_dir = dirname(rstudioapi::getSourceEditorContext()$path)
setwd(working_dir)

data <- read.csv("data2022_train.csv", colClasses=c("character", "numeric"))
```

```{r}
data_xts <- xts(data$y, as.POSIXct(data$X, format="%Y-%m-%d %H:%M:%S", tz="GMT"))
```

```{r}
periodicity(data_xts)
```

## 1. Data Exploration

```{r}
summary(data_xts)
```

### Plot time series

```{r}
plot(data_xts)
plot(apply.weekly(data_xts,FUN=mean))
plot(apply.monthly(data_xts,FUN=mean))

msts_cons <- data_xts[1:(144*60)] |> msts(seasonal.periods = c(144, 144*7))
p <- msts_cons |> mstl() |> autoplot()
p
ggsave('plots/ts_decomposition.jpg', p, height = 6 , width = 11)
```

```{r}
plot(data_xts[1:144])
plot(data_xts[1:144*21])
p <- autoplot(data_xts[1:144*7]) + xlab('Day') + ylab('Power')
p
ggsave('plots/one_week.jpg', p, height = 3 , width = 5)
```

### Box-Cox transformation

```{r}
med <- tapply(data_xts, rep(1:334, each = 144), mean)
sds <- tapply(data_xts, rep(1:334, each = 144), sd)
plot(med, sds) # 334 dots -> one for each day
```

```{r}
bc <- boxcox(sds ~ med)
lambda <- bc$x[which.max(bc$y)]
lambda
```

```{r}
data_xts_bc <- (data_xts^lambda - 1) / lambda

med_bc <- tapply(data_xts_bc, rep(1:334, each = 144), mean)
sds_bc <- tapply(data_xts_bc, rep(1:334, each = 144), sd)
```

```{r}
plot(med_bc, sds_bc) # 334 dots -> one for each day
plot(med, sds) # 334 dots -> one for each day
plot(data_xts_bc)

a <- data.frame(cbind(med, sds))
p <- ggplot(a, aes(x=med, y=sds)) + geom_point(size=0.8) + xlab('Mean') + ylab('Standard deviation')
p
ggsave('plots/scatter.jpg', p, height = 3 , width = 5)

a <- data.frame(cbind(med_bc, sds_bc))
p <- ggplot(a, aes(x=med_bc, y=sds_bc)) + geom_point(size=0.8) + xlab('Mean') + ylab('Standard deviation')
p
ggsave('plots/scatter_bc.jpg', p, height = 3 , width = 5)
```

### Detecting outliers

```{r}
plot(diff(data_xts,1))
plot(diff(data_xts_bc,1))
data_xts[diff(data_xts,1) < -5000 | diff(data_xts,1) > 5000] #|> index() |> as.Date()
```

```{r}
par(mfrow=c(2,2))
plot(data_xts["2017-01-14"], ylim=c(15000,50000), main='', yaxis.right = FALSE, major.ticks = "days", col="red")
plot(data_xts["2017-03-28"], ylim=c(15000,50000), main='', yaxis.right = FALSE, major.ticks = "days", col="red")
plot(data_xts["2017-04-20"], ylim=c(15000,50000), main='', yaxis.right = FALSE, major.ticks = "days", col="red")
plot(data_xts["2017-05-31"], ylim=c(15000,50000), main='', yaxis.right = FALSE, major.ticks = "days", col="red")

p <- autoplot(data_xts["2017-04-20"]) + xlab('Hour') + ylab('Power') + scale_x_datetime(date_labels =  "%H %M")
p
ggsave('plots/anomaly.jpg', p, height = 3 , width = 5)
```

### Train, validation e test set

```{r}
tseq <- seq(from = index(data_xts[nrow(data_xts),])+600, length.out = 144*30, by = 600)
data_xts_complete <- c(data_xts, xts(rep(as.numeric(NA), length(tseq)), tseq))
cat(paste0("from: ", index(data_xts_complete[1]), "\nto:   ", index(data_xts_complete[nrow(data_xts_complete)])))
```

```{r}
npred <- 30 # numero di giorni da predire
small_date <- "2017-10-01 00:00:00"
small_index <- which(index(data_xts_complete) == small_date)
val_date <- "2017-11-01 00:00:00"
val_index <- which(index(data_xts_complete) == val_date)
test_date <- "2017-12-01 00:00:00"
test_index <- which(index(data_xts_complete) == test_date)
cat(paste0("small_index: ", small_index,"\nval_index:   ", val_index, "\ntest_index:  ", test_index))
```

```{r}
train <- data_xts_complete[1:(val_index-1)]
small <- data_xts_complete[small_index:(val_index-1)] # train piu veloce
val <- data_xts_complete[val_index:(val_index+144*npred-1),]
test <- data_xts_complete[test_index:nrow(data_xts_complete)]
cat(paste0("small: ", nrow(small), "\ntrain: ", nrow(train), "\nval:   ", nrow(val), "\ntest:  ", nrow(test)))
```

## 2. Data Modeling

```{r}
stats_results <- data.frame(matrix(nrow=0,ncol=4))
colnames(stats_results)<-c("Model", "RMSE", "MAPE", "MAE")
```

### ARIMA Models

-   Possibilita 1

    -   modellare settimana con dummy

    -   modellare giorno con differenza stagionale

-   Possibilita 2

    -   144 modelli per togliere stagionalita giornaliera

-   Possibilita 3

    -   24 modelli per togliere la stagionalita giornaliera usando la media dell'ora

```{r}
train_dummy <- fastDummies::dummy_cols(format(index(train), "%u"), remove_selected_columns = TRUE, remove_first_dummy = TRUE)
rownames(train_dummy) <- index(train)
colnames(train_dummy) <- c("mart", "merc", "giov", "ven", "sab", "dom")
train_dummy <- as.matrix(train_dummy)

val_dummy <- fastDummies::dummy_cols(format(index(val), "%u"), remove_selected_columns = TRUE, remove_first_dummy = TRUE)
rownames(val_dummy) <- index(val)
colnames(val_dummy) <- c("mart", "merc", "giov", "ven", "sab", "dom")
val_dummy <- as.matrix(val_dummy)

#train_arima <- cbind(train, train_dummy)
```

Stimo i parametri (p, q, d)

```{r}
ndiffs(diff(train, 144))
nsdiffs(ts(train, frequency = 144))
```

```{r}
train |> Acf(300)
train |> #[42000:length(train)]
  #diff() |>
  diff() |>
  #diff(144*7) |>
  diff(144) |>
  #autoplot()
  Acf(300)
#train[diff(diff(train, 144), 1) < -4000]

p <- autoplot(Acf(train, 150)) + xlab('Lag') + ylab('ACF') + ggtitle('')
p
ggsave('plots/acf.jpg', p, height = 3 , width = 5)

p <- autoplot(Pacf(train, 150)) + xlab('Lag') + ylab('Partial ACF') + ggtitle('')
p
ggsave('plots/pacf.jpg', p, height = 3 , width = 5)
```

```{r}
train |> Pacf(300)
train |>
  diff(144) |>
  diff() |>
  Pacf(300)
```

```{r}
nsdiffs(train)
```

#### Mod 1

```{r}
#train <- (train^lambda - 1) / lambda
#data_xts <- (data_xts^lambda - 1) / lambda
```

```{r}
arima_mod1 <- function(train) {
  Arima(y = ts(train, freq = 144),
              order = c(0, 0, 0),
              seasonal = c(0, 1, 1),
              include.constant = FALSE)}
arima_mod1_train <- arima_mod1(train)
summary(arima_mod1_train)
```

```{r}
#arima_mod1.plot_diagnostics(figsize=c(7,5))
#plt.show()
```

```{r}
# plot(index(arima_mod1$residuals)/600/144, arima_mod1$residuals, type='l', xlab='Day', ylab='Residual')
```

```{r}
arima_pred1 <- forecast(arima_mod1_train, 144*npred)
plot(arima_pred1)
#abline(v=val_index*600)

arima_pred1 <- xts(arima_pred1$mean, index(val))
```

```{r}
plot_pred(arima_pred1)

stats_results[nrow(stats_results) + 1,] <- c("arima_mod1", stats(val, arima_pred1))
```

```{r}
Acf(diff(arima_pred1 - val, lag.max = 100))
```

#### Mod 2

auto.arima using "aic" without xreg: `ARIMA(2,0,2)(0,1,0)[144]`

```{r}
arima_mod2 <- function(train){
  auto.arima(y = ts(train, freq = 144),
             ic = 'aic',
             stepwise = FALSE,
             max.p = 2,
             max.q = 2,
             start.p = 0,
             start.q = 0,
             start.P = 0,
             start.Q = 0,)
}

#arima_mod2 <- function(train) {
#  Arima(y = ts(train, freq = 144),
#              order = c(0, 1, 1),
#              seasonal = c(0, 1, 1),
#              include.constant = FALSE)}
arima_mod2_train <- arima_mod2(train)
summary(arima_mod2_train)
```

```{r}
arima_pred2 <- forecast(arima_mod2_train, 144*npred)
plot(arima_pred2)

arima_pred2 <- xts(arima_pred2$mean, index(val))
```

```{r}
plot_pred(arima_pred2)

stats_results[nrow(stats_results) + 1,] <- c("arima_mod2", stats(val, arima_pred2))
```

#### Mod 3

auto.arima using "aic" with xreg: `ARIMA(1,0,2)(0,1,0)[144]`

```{r}
arima_mod3 <- function(train, train_dummy){
  auto.arima(y = ts(train, freq = 144),
             ic = 'aic',
             stepwise = FALSE,
             max.p = 2,
             max.q = 2,
             start.p = 0,
             start.q = 0,
             start.P = 0,
             start.Q = 0,
             xreg = train_dummy)
}

arima_mod3_train <- arima_mod3(train, train_dummy)
summary(arima_mod3_train)
```

```{r}
arima_pred3 <- forecast(arima_mod3_train,
                  144*npred)
plot(arima_pred3)
abline(v=val_index*600)

arima_pred3 <- xts(arima_pred3$mean, index(val))
```

```{r}
plot_pred(arima_pred3)

stats_results[nrow(stats_results) + 1,] <- c("arima_mod3", stats(val, arima_pred3))
```

#### Mod 4

```{r}
arima_mod4 <- function(train, train_dummy) {
  Arima(y = ts(train, freq = 144),
              order = c(0, 0, 0),
              seasonal = c(1, 0, 0),
              xreg = train_dummy,
              include.constant = FALSE)}
arima_mod4_train <- arima_mod4(train, train_dummy)
summary(arima_mod4_train)
```

```{r}
arima_pred4 <- forecast(arima_mod4_train,
                  144*npred,
                  xreg = val_dummy)
plot(arima_pred4)
abline(v=val_index*600)

arima_pred4 <- xts(arima_pred4$mean, index(val))
```

```{r}
plot_pred(arima_pred4)
stats_results[nrow(stats_results) + 1,] <- c("arima_mod4", stats(val, arima_pred4))

df_plot_pred <- merge.xts(ground_truth = data_xts[(val_index-144*4):nrow(data_xts),], pred = arima_pred4)
df_plot_pred <- data.frame(df_plot_pred, date=index(df_plot_pred))

p <- ggplot() + 
  geom_line(data = df_plot_pred, aes(x = date, y = ground_truth)) +
  geom_line(data = df_plot_pred, aes(x = date, y = pred), color = "#F8766D") +
  geom_vline(xintercept=as.numeric(df_plot_pred$date[144*4]), linetype=2) +
  xlab('Date') +
  ylab('Power')
p
ggsave('plots/arima_pred.jpg', p, height = 2 , width = 6)
```

#### Mod 5

```{r}
train_meanhour <- period.apply(train, endpoints(train, "hours"), mean)
index(train_meanhour) <- index(train_meanhour)-600*2

train_byhour <- vector('list', 24)
for (hour in 1:24) {
  train_byhour[[hour]] <- train_meanhour[seq(hour, length(train_meanhour), 24)]
  }
```

```{r}
val_meanhour <- period.apply(val, endpoints(val, "hours"), mean)
index(val_meanhour) <- index(val_meanhour)-600*2

val_byhour <- vector('list', 24)
for (hour in 1:24) {
  val_byhour[[hour]] <- val_meanhour[seq(hour, length(val_meanhour), 24)] # to obtain the index
  }
```

```{r}
plot(train_byhour[[9]])

train_byhour[[9]] |> diff() |> Pacf()
```

```{r}
arima_mod5_byhour <- vector('list', 24)
for (hour in 1:24) {
  arima_mod5_byhour[[hour]] <- Arima(y = ts(train_byhour[[hour]], freq = 7),
                              order = c(0, 1, 1),
                              seasonal = c(0, 1, 1),
                              include.constant = FALSE
                              )
  }
```

```{r}
# auto arima
# mod5_byhour <- vector('list', 24)
# for (hour in 1:24) {
#   print(hour)
#   mod5_byhour[[hour]] <- auto.arima(y = ts(train_byhour[[hour]], freq = 7),
#                                     )
#   }
```

```{r}
arima_pred5_byhour <- vector('list', 24)
for (hour in 1:24) {
  arima_pred5_byhour[[hour]] <- forecast(arima_mod5_byhour[[hour]], npred)
  arima_pred5_byhour[[hour]] <- xts(arima_pred5_byhour[[hour]]$mean, index(val_byhour[[hour]]))
}

arima_pred5_merged <- do.call(rbind, arima_pred5_byhour) # concatenate the predictions
```

```{r}
temp <- xts(rep(as.numeric(NA), length(val)), index(val))
arima_pred5 <- merge(temp, arima_pred5_merged)$arima_pred5_merged
arima_pred5 <- na.approx(arima_pred5, na.rm = FALSE)
arima_pred5 <- na.locf(arima_pred5, na.rm = FALSE)
arima_pred5 <- na.locf(arima_pred5, fromLast = TRUE)
names(arima_pred5) <- "V1"
```

```{r}
arima_stats5_byhour <- vector('list', 24)
for (hour in 1:24) {
  arima_stats5_byhour[[hour]] <- stats(arima_pred5_byhour[[hour]], val_byhour[[hour]])[[3]]
}
plot(unlist(arima_stats5_byhour), type='l', ylab = 'MAE', xlab = 'Hour')


p <- ggplot(data = data.frame(x=1:24, y=unlist(arima_stats5_byhour)), aes(x=x, y=y)) +
  geom_point() + 
  geom_segment( aes(x=x, xend=x, y=0, yend=y)) + xlab('Hours') + ylab('MAE')
p
ggsave('plots/mae_perhour.jpg', p, height = 3 , width = 5)
  
```

```{r}
plot_pred(arima_pred5)
stats_results[nrow(stats_results) + 1,] <- c("arima_mod5", stats(val, arima_pred5))
```

### UCM Models

Definisco il train set per i modelli ucm e una versione ridotta per velocizzare il train

```{r}
tseq <- seq(from = index(train[nrow(train),])+600,length.out = 144*npred,by = 600) # creo sequenza di NA a step di 10 min
train_ucm <- c(train, xts(rep(as.numeric(NA), length(tseq)), tseq)) # aggiungo NA a train
small_ucm <- c(small, xts(rep(as.numeric(NA), length(tseq)), tseq)) # aggiungo NA a small

train_dummy_ucm <- fastDummies::dummy_cols(format(index(train_ucm), "%u"), remove_selected_columns = TRUE, remove_first_dummy = TRUE)
rownames(train_dummy_ucm) <- index(train_ucm)
colnames(train_dummy_ucm) <- c("mart", "merc", "giov", "ven", "sab", "dom")
train_dummy_ucm <- as.matrix(train_dummy_ucm)
```

#### Mod 1

```{r}
ucm_mod1 <- function(train) {
  # Definizione del modello
  mod <- SSModel(V1~SSMtrend(2, list(NA, NA))+
                    SSMseasonal(144, NA, "trigonometric", harmonics = 1:2)+
                    SSMseasonal(1008, NA, "trigonometric", harmonics = 1),
                  H=NA, 
                  data=train)

  # Assegna i valori iniziali ai parametri, parte da 0.
  mod$P1inf[] <- 0 # no componenti diffuse
  mod$a1[1] <- mean(log(train[1:144])) # scelta basata sui valori del primo mese
  
  vy <- var(log(train[1:144])) # varianza serie storica (utilizzata sotto per dare un ordine di grandezza)
  diag(mod$P1) <- vy*10 # specifichiamo che abbiamo molta incertezza sui valori che abbiamo specificato
  
  #Inizializzazione delle varianze sulla base di vy
  pars <- log(c(
    logVarEta = vy/100000,
    logVarZeta = vy/150,
    logVarOm144 = vy,
    logVarOm1008 = vy/100,
    logVarEps = vy/10
  ))
  
  # funzione di update
  updt <- function(pars, model){
    model$Q[1,1,1] <- exp(pars[1])
    model$Q[2,2,1] <- exp(pars[2])
    diag(model$Q[3:6, 3:6, 1]) <- exp(pars[3])
    diag(model$Q[7:8, 7:8, 1]) <- exp(pars[4])
    model$H[1,1,1] <- exp(pars[5])
    model
  }
  
  # Train - Si allena sui valori passati (quindi quei valori di train non nulli)
  fit <- fitSSM(mod, pars, updt)
  fit$optim.out
  
  # Filtro di karman - Effetua le predizioni - kfs1$muhat contiene una serie storica predetta (anche i dati di train vengono predetti)
  kfs <- KFS(fit$model,
            smoothing = c("state", "signal", "disturbance"))
  
  # conversione muhat in serie storica
  muhat <- xts(as.matrix(kfs$muhat),
                index(train))
  muhat <- as.xts(muhat)
  return(muhat)
}
```

```{r}
ucm_pred1 <- ucm_mod1(train_ucm)
```

```{r}
plot_pred(ucm_pred1)

ucm_stats1 <- stats(data_xts[val_index:(val_index+144*npred-1)], ucm_pred1[(nrow(ucm_pred1)-144*npred+1):nrow(ucm_pred1)])
stats_results[nrow(stats_results) + 1,] <- c("ucm_mod1", ucm_stats1)
```

#### Mod 2

```{r}
ucm_mod2 <- function(train) {
  arm144 <- 10
  arm1008 <- 1
  # Definizione del modello
  mod <- SSModel(V1~SSMtrend(2, list(NA, NA))+
                    SSMseasonal(144, NA, "trigonometric", harmonics = 1:arm144)+
                    SSMseasonal(1008, NA, "trigonometric", harmonics = 1:arm1008),
                  H=NA, 
                  data=train)

  # Assegna i valori iniziali ai parametri, parte da 0.
  mod$P1inf[] <- 0 # no componenti diffuse
  mod$a1[1] <- mean(train[1:144]) # expected value of the initial state vector Î±[1]
  
  vy <- var(train[1:144]) # varianza serie storica (utilizzata sotto per dare un ordine di grandezza)
  diag(mod$P1) <- vy*10 # specifichiamo che abbiamo molta incertezza sui valori che abbiamo specificato
  
  #Inizializzazione delle varianze sulla base di vy
  pars <- log(c(
    logVarEta = vy/10000,
    logVarZeta = vy/100000,
    logVarOm144 = vy/1000,
    logVarOm1008 = vy/100000,
    logVarEps = vy/1000
  ))
  
  # funzione di update
  updt <- function(pars, model){
    model$Q[1,1,1] <- exp(pars[1]) # level
    model$Q[2,2,1] <- exp(pars[2]) # slope
    diag(model$Q[3:(2+arm144*2), 3:(2+arm144*2), 1]) <- exp(pars[3]) # seasonality 144
    diag(model$Q[(3+arm144*2):(2+arm144*2+arm1008*2), (3+arm144*2):(2+arm144*2+arm1008*2), 1]) <- exp(pars[4]) # seasonality 1008
    model$H[1,1,1] <- exp(pars[5])
    model
  }
  
  # Train - Si allena sui valori passati (quindi quei valori di train non nulli)
  fit <- fitSSM(mod, pars, updt)
  print(fit$optim.out)
  
  # Filtro di karman - Effetua le predizioni - kfs1$muhat contiene una serie storica predetta (anche i dati di train vengono predetti)
  kfs <- KFS(fit$model,
             smoothing = c("state", "signal", "disturbance"))
  
  # conversione muhat in serie storica
  muhat <- xts(as.matrix(kfs$muhat),
                index(train))
  muhat <- as.xts(muhat)
  return(muhat)
}
```

```{r}
ucm_pred2 <- ucm_mod2(train_ucm)
```

```{r}
plot_pred(ucm_pred2)

ucm_stats2 <- stats(data_xts[val_index:(val_index+144*npred-1)], ucm_pred2[(nrow(ucm_pred2)-144*npred+1):nrow(ucm_pred2)])
stats_results[nrow(stats_results) + 1,] <- c("ucm_mod2", ucm_stats2)

df_plot_pred <- merge.xts(ground_truth = data_xts[(val_index-144*4):nrow(data_xts),], pred = ucm_pred2[val_index:nrow(data_xts),])
df_plot_pred <- data.frame(df_plot_pred, date=index(df_plot_pred))

p <- ggplot() + 
  geom_line(data = df_plot_pred, aes(x = date, y = ground_truth)) +
  geom_line(data = df_plot_pred, aes(x = date, y = pred), color = "#619CFF") +
  geom_vline(xintercept=as.numeric(df_plot_pred$date[144*4]), linetype=2) +
  xlab('Date') +
  ylab('Power')
p
ggsave('plots/ucm_pred.jpg', p, height = 2 , width = 6)
```

Minimo

    RMSE 1718.399 
    MAPE 4.814626 
    MAE  1366.986

#### Mod 3

```{r}

ucm_mod3 <- function(train, train_dummy) {
  arm144 <- 10
  arm1008 <- 1
  # Definizione del modello
  mod <- SSModel(V1~SSMtrend(2, list(NA, NA))+
                    SSMseasonal(144, NA, "trigonometric", harmonics = 1:arm144)+
                    # SSMseasonal(1008, NA, "trigonometric", harmonics = 1:arm1008),
                    train_dummy,
                 H=NA, 
                 data=train)

  # Assegna i valori iniziali ai parametri, parte da 0.
  mod$P1inf[] <- 0 # no componenti diffuse
  mod$a1[1] <- mean(train[1:144]) # expected value of the initial state vector Î±[1]
  
  vy <- var(train[1:144]) # varianza serie storica (utilizzata sotto per dare un ordine di grandezza)
  diag(mod$P1) <- vy*10 # specifichiamo che abbiamo molta incertezza sui valori che abbiamo specificato
  
  #Inizializzazione delle varianze sulla base di vy
  pars <- log(c(
    logVarEta = vy/10000,
    logVarZeta = vy/100000,
    logVarOm144 = vy/1000,
    #logVarOm1008 = vy/100000,
    logVarEps = vy/1000
  ))
  
  # funzione di update
  updt <- function(pars, model){
    model$Q[1,1,1] <- exp(pars[1]) # level
    model$Q[2,2,1] <- exp(pars[2]) # slope
    diag(model$Q[3:(2+arm144*2), 3:(2+arm144*2), 1]) <- exp(pars[3]) # seasonality 144
    #diag(model$Q[(3+arm144*2):(2+arm144*2+arm1008*2), (3+arm144*2):(2+arm144*2+arm1008*2), 1]) <- exp(pars[4]) # seasonality 1008
    model$H[1,1,1] <- exp(pars[4]) # H
    model
  }
  
  # Train - Si allena sui valori passati (quindi quei valori di train non nulli)
  fit <- fitSSM(mod, pars, updt)
  print(fit$optim.out)
  
  # Filtro di kalman - Effetua le predizioni - kfs1$muhat contiene una serie storica predetta (anche i dati di train vengono predetti)
  kfs <- KFS(fit$model,
            smoothing = c("state", "signal", "disturbance"))
  
  # conversione muhat in serie storica
  muhat <- xts(as.matrix(kfs$muhat),
                index(train))
  muhat <- as.xts(muhat)
  return(muhat)
}
```

```{r}
ucm_pred3 <- ucm_mod3(train_ucm, train_dummy_ucm)
```

```{r}
plot_pred(ucm_pred3)

ucm_stats3 <- stats(data_xts[val_index:(val_index+144*npred-1)], ucm_pred3[(nrow(ucm_pred3)-144*npred+1):nrow(ucm_pred3)])
stats_results[nrow(stats_results) + 1,] <- c("ucm_mod3", ucm_stats3)
```

#### Mod 4

```{r}
ucm_mod4 <- function(train) {
  arm7 <- 10
  # Definizione del modello
  mod <- SSModel(V1~SSMtrend(2, list(NA, NA))+
                    SSMseasonal(7, NA, "dummy"),
                    # SSMseasonal(1008, NA, "trigonometric", harmonics = 1:arm1008),
                    # train_dummy,
                 H=NA, 
                 data=train)

  # Assegna i valori iniziali ai parametri, parte da 0.
  mod$P1inf[] <- 0 # no componenti diffuse
  mod$a1[1] <- mean(train[1:14]) # expected value of the initial state vector Î±[1]
  
  vy <- var(train[1:14]) # varianza serie storica (utilizzata sotto per dare un ordine di grandezza)
  diag(mod$P1) <- vy*10 # specifichiamo che abbiamo molta incertezza sui valori che abbiamo specificato
  
  #Inizializzazione delle varianze sulla base di vy
  pars <- log(c(
    logVarEta = vy/10000,
    logVarZeta = vy/100000,
    logVarOm7 = vy/1000,
    #logVarOm1008 = vy/100000,
    logVarEps = vy/1000
  ))
  
  # funzione di update
  updt <- function(pars, model){
    model$Q[1,1,1] <- exp(pars[1]) # level
    model$Q[2,2,1] <- exp(pars[2]) # slope
    diag(model$Q[3:(2+arm144*2), 3:(2+arm144*2), 1]) <- exp(pars[3]) # seasonality 144
    #diag(model$Q[(3+arm144*2):(2+arm144*2+arm1008*2), (3+arm144*2):(2+arm144*2+arm1008*2), 1]) <- exp(pars[4]) # seasonality 1008
    model$H[1,1,1] <- exp(pars[4]) # H
    model
  }
  
  # Train - Si allena sui valori passati (quindi quei valori di train non nulli)
  fit <- fitSSM(mod, pars)
  print(fit$optim.out)
  
  # Filtro di kalman - Effetua le predizioni - kfs1$muhat contiene una serie storica predetta (anche i dati di train vengono predetti)
  kfs <- KFS(fit$model,
            smoothing = c("state", "signal", "disturbance"))
  
  # conversione muhat in serie storica
  muhat <- xts(as.matrix(kfs$muhat),
                index(train))
  muhat <- as.xts(muhat)
  return(muhat)
}
```

```{r}
train_meanhour_ucm <- period.apply(train_ucm, endpoints(train_ucm, "hours"), mean)
index(train_meanhour_ucm) <- index(train_meanhour_ucm)-600*2

train_byhour_ucm <- vector('list', 24)
for (hour in 1:24) {
  train_byhour_ucm[[hour]] <- train_meanhour_ucm[seq(hour, length(train_meanhour_ucm), 24)]
  }
```

```{r}
val_meanhour_ucm <- period.apply(data_xts, endpoints(data_xts, "hours"), mean)
index(val_meanhour_ucm) <- index(val_meanhour_ucm)-600*2

val_byhour_ucm <- vector('list', 24)
for (hour in 1:24) {
  val_byhour_ucm[[hour]] <- val_meanhour_ucm[seq(hour, length(val_meanhour_ucm), 24)] # to obtain the index
  }
```

```{r}
ucm_mod4_byhour <- vector('list', 24)
for (hour in 1:24) {
  ucm_mod4_byhour[[hour]] <- ucm_mod4(train_byhour_ucm[[hour]])
  }
```

```{r}
ucm_pred4_byhour <- vector('list', 24)
for (hour in 1:24) {
  ucm_pred4_byhour[[hour]] <- forecast(ucm_mod4_byhour[[hour]], npred)
  ucm_pred4_byhour[[hour]] <- xts(ucm_pred4_byhour[[hour]]$mean, index(val_byhour[[hour]]))
}

ucm_pred4_merged <- do.call(rbind, ucm_pred4_byhour) # concatenate the predictions
```

```{r}
temp <- xts(rep(as.numeric(NA), length(val)), index(val))
arima_pred5 <- merge(temp, arima_pred5_merged)$arima_pred5_merged
arima_pred5 <- na.approx(arima_pred5, na.rm = FALSE)
arima_pred5 <- na.locf(arima_pred5, na.rm = FALSE)
arima_pred5 <- na.locf(arima_pred5, fromLast = TRUE)
names(arima_pred5) <- "V1"
```

```{r}
arima_stats5_byhour <- vector('list', 24)
for (hour in 1:24) {
  arima_stats5_byhour[[hour]] <- stats(arima_pred5_byhour[[hour]], val_byhour[[hour]])[[3]]
}
plot(unlist(arima_stats5_byhour), type='l', ylab = 'MAE', xlab = 'Hour')


p <- ggplot(data = data.frame(x=1:24, y=unlist(arima_stats5_byhour)), aes(x=x, y=y)) +
  geom_point() + 
  geom_segment( aes(x=x, xend=x, y=0, yend=y)) + xlab('Hours') + ylab('MAE')
p
ggsave('plots/mae_perhour.jpg', p, height = 3 , width = 5)
  
```

```{r}
plot_pred(arima_pred5)
stats_results[nrow(stats_results) + 1,] <- c("arima_mod5", stats(val, arima_pred5))
```

#### Mod 5

```{r}
ucm_mod5 <- function(train) {
  arm144 <- 10
  arm1008 <- 1
  # Definizione del modello
  mod <- SSModel(V1~SSMtrend(2, list(NA, NA))+
                    SSMseasonal(144, NA, "trigonometric", harmonics = 1:arm144)+
                    #SSMseasonal(1008, NA, "trigonometric", harmonics = 1:arm1008),
                    SSMcycle(1008, NA),
                  H=NA, 
                  data=train)

  # Assegna i valori iniziali ai parametri, parte da 0.
  mod$P1inf[] <- 0 # no componenti diffuse
  mod$a1[1] <- mean(train[1:144]) # expected value of the initial state vector Î±[1]
  
  vy <- var(train[1:144]) # varianza serie storica (utilizzata sotto per dare un ordine di grandezza)
  diag(mod$P1) <- vy*10 # specifichiamo che abbiamo molta incertezza sui valori che abbiamo specificato
  
  #Inizializzazione delle varianze sulla base di vy
  pars <- log(c(
    logVarEta = vy/10000,
    logVarZeta = vy/100000,
    logVarOm144 = vy/1000,
    logVarOm10 = vy/100000,
    logVarEps = vy/1000
  ))
  
  # funzione di update
  updt <- function(pars, model){
    model$Q[1,1,1] <- exp(pars[1]) # level
    model$Q[2,2,1] <- exp(pars[2]) # slope
    diag(model$Q[3:(2+arm144*2), 3:(2+arm144*2), 1]) <- exp(pars[3]) # seasonality 144
    #diag(model$Q[(3+arm144*2):(2+arm144*2+arm1008*2), (3+arm144*2):(2+arm144*2+arm1008*2), 1]) <- exp(pars[4]) # seasonality 1008
    model$Q[23:24, 23:24, 1] <- exp(pars[4])
    model$H[1,1,1] <- exp(pars[5])
    model
  }
  
  # Train - Si allena sui valori passati (quindi quei valori di train non nulli)
  fit <- fitSSM(mod, pars, updt)
  print(fit$optim.out)
  
  # Filtro di karman - Effetua le predizioni - kfs1$muhat contiene una serie storica predetta (anche i dati di train vengono predetti)
  kfs <- KFS(fit$model,
             smoothing = c("state", "signal", "disturbance"))
  
  # conversione muhat in serie storica
  muhat <- xts(as.matrix(kfs$muhat),
                index(train))
  muhat <- as.xts(muhat)
  return(muhat)
}
```

```{r}
ucm_pred5 <- ucm_mod5(train_ucm)
```

```{r}
plot_pred(ucm_pred5)

ucm_stats5 <- stats(data_xts[val_index:(val_index+144*npred-1)], ucm_pred5[(nrow(ucm_pred5)-144*npred+1):nrow(ucm_pred5)])
stats_results[nrow(stats_results) + 1,] <- c("ucm_mod5", ucm_stats5)
```

### Machine Learning

La serie Ã© abbastanza regolare quindi (a meno di eventi imprevedibili) le predizioni molto probabilemtne ircadono nel range dei valori passati e ne ricalcano i trend

#### Mod 1

```{r}
ml_mod1 <- function(train){
  knn_forecasting(ts(train),
                      h = 144*npred,
                      lags = 1:(144*7),
                      #k = 2,
                      msas = "MIMO",# cf = "median",
                      transform = "multiplicative")}
```

```{r}
ml_pred1 <- ml_mod1(train)
ml_pred1 <- xts(ml_pred1$prediction, index(val))
```

```{r}
plot_pred(ml_pred1)
stats_results[nrow(stats_results) + 1,] <- c("ml_mod1", stats(val, ml_pred1))


df_plot_pred <- merge.xts(ground_truth = data_xts[(val_index-144*4):nrow(data_xts),], pred = ml_pred1)
df_plot_pred <- data.frame(df_plot_pred, date=index(df_plot_pred))

p <- ggplot() + 
  geom_line(data = df_plot_pred, aes(x = date, y = ground_truth)) +
  geom_line(data = df_plot_pred, aes(x = date, y = pred), color = "#00BA38") +
  geom_vline(xintercept=as.numeric(df_plot_pred$date[144*4]), linetype=2) +
  xlab('Date') +
  ylab('Power')
p
ggsave('plots/ml_pred.jpg', p, height = 2 , width = 6)
```

#### Mod 2

```{r}
ml_mod2 <- function(train){
  knn_forecasting(ts(train),
                      h = 144*npred,
                      lags = 1:(144),
                      #k = 2,
                      msas = "MIMO",# cf = "median",
                      transform = "multiplicative")}
```

```{r}
ml_pred2 <- ml_mod2(train)
ml_pred2 <- xts(ml_pred2$prediction, index(val))
```

```{r}
plot_pred(ml_pred2)
stats_results[nrow(stats_results) + 1,] <- c("ml_mod2", stats(val, ml_pred2))
```

#### Mod 3

```{r}
num_lags <- 24*7

# aggregate data by hour
data_xts_agg <- period.apply(data_xts, endpoints(data_xts, "hours"), mean)
index(data_xts_agg) <- index(data_xts_agg)-600*2

wday <- wday(time(data_xts_agg), week_start = getOption("lubridate.week.start", 1)) # day of the week
lags <- embed(data_xts_agg, (num_lags+1))[,-1]

y_ml <- data_xts_agg[-(1:num_lags)]
x_ml <- cbind(wday[-(1:num_lags)], lags)

# split in train and val
y_ml_train <- y_ml[1:as.integer(val_index/6-num_lags)]
x_ml_train <- x_ml[1:as.integer(val_index/6-num_lags),]
y_ml_val <- y_ml[as.integer(val_index/6-(num_lags-1)):nrow(y_ml)]
x_ml_val <- x_ml[as.integer(val_index/6-(num_lags-1)):nrow(y_ml),]
```

```{r}
# training
ml_mod3 <- randomForest(x_ml_train, y_ml_train,
                    ntree=200)

# predictions
ml_pred3 <- c()
elem <- x_ml_val[1,] # first elem of val

for (i in 1:nrow(y_ml_val)){
  pr <- predict(ml_mod3, newdata = elem)
  elem <- elem[-length(elem)] # remove last elem
  elem <- append(elem, pr, after=1) # add new elem
  ml_pred3 <- append(ml_pred3, pr)
}

ml_pred3 <- as.xts(ml_pred3, order.by = index(y_ml_val))

# imputation
temp <- xts(rep(as.numeric(NA), length(val)), index(val))
ml_pred3 <- merge(temp, ml_pred3)$ml_pred3
ml_pred3 <- na.approx(ml_pred3, na.rm = FALSE)
ml_pred3 <- na.locf(ml_pred3, na.rm = FALSE)
ml_pred3 <- na.locf(ml_pred3, fromLast = TRUE)
names(ml_pred3) <- "V1"
```

```{r}
plot_pred(ml_pred3)
stats_results[nrow(stats_results) + 1,] <- c("ml_mod3", stats(val, ml_pred3))
```

implementation with ranger

```{r}
# oo <- data.frame(y_ml_train, x_ml_train)
# ooo <- data.frame(y_ml_val, x_ml_val)
# ml_mod3 <- ranger(formula = y_ml_train ~ .,
#                     data = oo,
#                     num.trees=100)
```

```{r}
# # predictions
# ml_pred3 <- c()
# elem <- ooo[1,-1] # first elem of val
# 
# for (i in 1:nrow(y_ml_val)){
#   pr <- predict(ml_mod3, data = elem)$predictions
#   elem <- elem[,-length(elem)] # remove last elem
#   add_column(elem, d = pr, .after = 1)
#   #elem <- append(elem, pr, after=3) # add new elem
#   ml_pred3 <- append(ml_pred3, pr)
# }
# # giusto in giu
# ml_pred3 <- as.xts(ml_pred3, order.by = index(y_ml_val))
# 
# temp <- xts(rep(as.numeric(NA), length(val)), index(val))
# 
# ml_pred3 <- merge(temp, ml_pred3)$ml_pred3
# ml_pred3 <- na.approx(ml_pred3, na.rm = FALSE)
# ml_pred3 <- na.locf(ml_pred3, na.rm = FALSE)
# ml_pred3 <- na.locf(ml_pred3, fromLast = TRUE)
# names(ml_pred3) <- "V1"
# 
# stats(val, ml_pred3)
# plot_pred(ml_pred3)
```

## 3. Data prediction

```{r}
train_gen_nov <- data_xts_complete[1:(val_index+144*npred-1),]

# arima train
train_gen_nov_dummy <- fastDummies::dummy_cols(format(index(train_gen_nov), "%u"), remove_selected_columns = TRUE, remove_first_dummy = TRUE)
rownames(train_gen_nov_dummy) <- index(train_gen_nov)
colnames(train_gen_nov_dummy) <- c("mart", "merc", "giov", "ven", "sab", "dom")
train_gen_nov_dummy <- as.matrix(train_gen_nov_dummy)

test_gen_nov_dummy <- fastDummies::dummy_cols(format(index(test), "%u"), remove_selected_columns = TRUE, remove_first_dummy = TRUE)
rownames(test_gen_nov_dummy) <- index(test)
colnames(test_gen_nov_dummy) <- c("mart", "merc", "giov", "ven", "sab", "dom")
test_gen_nov_dummy <- as.matrix(test_gen_nov_dummy)

# ucm train
tseq <- seq(from = index(train_gen_nov[nrow(train_gen_nov),])+600, length.out = 144*npred, by = 600)
train_gen_nov_ucm <- c(train_gen_nov, xts(rep(as.numeric(NA), length(tseq)), tseq)) # aggiungo NA a train
```

```{r}
stats_results
```

#### ARIMA

```{r}
dic_arima_train <- arima_mod4(train_gen_nov, train_gen_nov_dummy)

dic_arima_pred <- forecast(dic_arima_train, 144*npred, xreg=test_gen_nov_dummy)
dic_arima_pred <- xts(dic_arima_pred$mean, index(test))
```

#### UCM

```{r}
dic_ucm <- ucm_mod2(train_gen_nov_ucm) # modello 1

dic_ucm_pred <- dic_ucm[(nrow(dic_ucm)-144*npred+1):nrow(dic_ucm)]
```

#### ML

```{r}
dic_ml <- ml_mod1(train_gen_nov)$prediction # modello 1

dic_ml_pred <- xts(dic_ml, index(test))
```

#### Results

```{r}
results <- data.frame(date = index(test),
                      ARIMA = dic_arima_pred,
                      UCM = dic_ucm_pred,
                      ML = dic_ml_pred)
```

```{r}
sum(results$ARIMA)
sum(results$UCM)
sum(results$ML)
```

```{r}
df_plot_pred <- merge.xts(Actual = data_xts_complete[(test_index-144*4):nrow(data_xts_complete),], ARIMA = dic_arima_pred)
df_plot_pred <- merge.xts(df_plot_pred, UCM = dic_ucm_pred)
df_plot_pred <- merge.xts(df_plot_pred, SVM = dic_ml_pred)

df_plot_pred <- data.frame(df_plot_pred, date=index(df_plot_pred))
df <- gather(df_plot_pred, Model, Values, Actual:SVM)

p <- ggplot(data = df, aes(x=date, y=Values)) +
  geom_line(aes(color = Model), size = 0.5) +
  labs(y = "Power", x = '') + 
  scale_color_manual(values=c("black", "#F8766D", "#00BA38", "#619CFF")) +
  geom_vline(xintercept=as.numeric(df_plot_pred$date[144*4]), linetype=2)
p

ggsave('plots/december_forecasts_comparison.jpg', p, height = 3 , width = 3 * 4)
```

```{r}
write.csv(results, "886725_20230208.csv", row.names=FALSE) # cambiare data
```
